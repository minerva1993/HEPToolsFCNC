                         : Option SaveBestOnly: Only model weights with smallest validation loss will be stored
keras_full1.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.
  #!/usr/bin/env python
Train on 475000 samples, validate on 187095 samples
Epoch 1/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.4197 - binary_accuracy: 0.9237Epoch 00000: val_loss improved from inf to 0.03590, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 152s - loss: 0.4196 - binary_accuracy: 0.9238 - val_loss: 0.0359 - val_binary_accuracy: 0.8918
Epoch 2/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2555 - binary_accuracy: 0.9264Epoch 00001: val_loss improved from 0.03590 to 0.02781, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 146s - loss: 0.2555 - binary_accuracy: 0.9264 - val_loss: 0.0278 - val_binary_accuracy: 0.8923
Epoch 3/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2491 - binary_accuracy: 0.9265Epoch 00002: val_loss improved from 0.02781 to 0.02562, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 162s - loss: 0.2491 - binary_accuracy: 0.9265 - val_loss: 0.0256 - val_binary_accuracy: 0.8925
Epoch 4/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2461 - binary_accuracy: 0.9265Epoch 00003: val_loss improved from 0.02562 to 0.02456, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 153s - loss: 0.2461 - binary_accuracy: 0.9265 - val_loss: 0.0246 - val_binary_accuracy: 0.8926
Epoch 5/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2441 - binary_accuracy: 0.9266Epoch 00004: val_loss improved from 0.02456 to 0.02389, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 146s - loss: 0.2441 - binary_accuracy: 0.9266 - val_loss: 0.0239 - val_binary_accuracy: 0.8928
Epoch 6/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2433 - binary_accuracy: 0.9267Epoch 00005: val_loss improved from 0.02389 to 0.02350, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2433 - binary_accuracy: 0.9267 - val_loss: 0.0235 - val_binary_accuracy: 0.8927
Epoch 7/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2421 - binary_accuracy: 0.9267Epoch 00006: val_loss improved from 0.02350 to 0.02320, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2422 - binary_accuracy: 0.9267 - val_loss: 0.0232 - val_binary_accuracy: 0.8929
Epoch 8/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2415 - binary_accuracy: 0.9267Epoch 00007: val_loss improved from 0.02320 to 0.02296, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2416 - binary_accuracy: 0.9267 - val_loss: 0.0230 - val_binary_accuracy: 0.8931
Epoch 9/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2405 - binary_accuracy: 0.9268Epoch 00008: val_loss improved from 0.02296 to 0.02277, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2406 - binary_accuracy: 0.9268 - val_loss: 0.0228 - val_binary_accuracy: 0.8932
Epoch 10/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2403 - binary_accuracy: 0.9269Epoch 00009: val_loss improved from 0.02277 to 0.02264, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2403 - binary_accuracy: 0.9269 - val_loss: 0.0226 - val_binary_accuracy: 0.8931
Epoch 11/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2394 - binary_accuracy: 0.9269Epoch 00010: val_loss improved from 0.02264 to 0.02250, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2394 - binary_accuracy: 0.9269 - val_loss: 0.0225 - val_binary_accuracy: 0.8933
Epoch 12/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2390 - binary_accuracy: 0.9269Epoch 00011: val_loss improved from 0.02250 to 0.02244, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2390 - binary_accuracy: 0.9269 - val_loss: 0.0224 - val_binary_accuracy: 0.8933
Epoch 13/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2388 - binary_accuracy: 0.9270Epoch 00012: val_loss improved from 0.02244 to 0.02233, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2388 - binary_accuracy: 0.9270 - val_loss: 0.0223 - val_binary_accuracy: 0.8933
Epoch 14/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2384 - binary_accuracy: 0.9269Epoch 00013: val_loss improved from 0.02233 to 0.02221, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2384 - binary_accuracy: 0.9269 - val_loss: 0.0222 - val_binary_accuracy: 0.8932
Epoch 15/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2383 - binary_accuracy: 0.9270Epoch 00014: val_loss improved from 0.02221 to 0.02216, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2383 - binary_accuracy: 0.9271 - val_loss: 0.0222 - val_binary_accuracy: 0.8932
Epoch 16/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2380 - binary_accuracy: 0.9270Epoch 00015: val_loss improved from 0.02216 to 0.02211, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2380 - binary_accuracy: 0.9270 - val_loss: 0.0221 - val_binary_accuracy: 0.8936
Epoch 17/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2375 - binary_accuracy: 0.9271Epoch 00016: val_loss improved from 0.02211 to 0.02205, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2375 - binary_accuracy: 0.9271 - val_loss: 0.0221 - val_binary_accuracy: 0.8934
Epoch 18/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2375 - binary_accuracy: 0.9271Epoch 00017: val_loss improved from 0.02205 to 0.02200, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2375 - binary_accuracy: 0.9271 - val_loss: 0.0220 - val_binary_accuracy: 0.8934
Epoch 19/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2371 - binary_accuracy: 0.9271Epoch 00018: val_loss improved from 0.02200 to 0.02197, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2371 - binary_accuracy: 0.9271 - val_loss: 0.0220 - val_binary_accuracy: 0.8936
Epoch 20/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2371 - binary_accuracy: 0.9272Epoch 00019: val_loss improved from 0.02197 to 0.02191, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2370 - binary_accuracy: 0.9272 - val_loss: 0.0219 - val_binary_accuracy: 0.8935
Epoch 21/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2369 - binary_accuracy: 0.9271Epoch 00020: val_loss improved from 0.02191 to 0.02186, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2369 - binary_accuracy: 0.9271 - val_loss: 0.0219 - val_binary_accuracy: 0.8937
Epoch 22/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2367 - binary_accuracy: 0.9271Epoch 00021: val_loss improved from 0.02186 to 0.02184, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2367 - binary_accuracy: 0.9271 - val_loss: 0.0218 - val_binary_accuracy: 0.8938
Epoch 23/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2367 - binary_accuracy: 0.9272Epoch 00022: val_loss improved from 0.02184 to 0.02181, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2367 - binary_accuracy: 0.9272 - val_loss: 0.0218 - val_binary_accuracy: 0.8936
Epoch 24/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2365 - binary_accuracy: 0.9272Epoch 00023: val_loss improved from 0.02181 to 0.02180, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2365 - binary_accuracy: 0.9272 - val_loss: 0.0218 - val_binary_accuracy: 0.8934
Epoch 25/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2361 - binary_accuracy: 0.9272Epoch 00024: val_loss improved from 0.02180 to 0.02176, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2361 - binary_accuracy: 0.9272 - val_loss: 0.0218 - val_binary_accuracy: 0.8936
Epoch 26/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2364 - binary_accuracy: 0.9272Epoch 00025: val_loss improved from 0.02176 to 0.02173, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2363 - binary_accuracy: 0.9272 - val_loss: 0.0217 - val_binary_accuracy: 0.8935
Epoch 27/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2361 - binary_accuracy: 0.9273Epoch 00026: val_loss improved from 0.02173 to 0.02168, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2362 - binary_accuracy: 0.9272 - val_loss: 0.0217 - val_binary_accuracy: 0.8937
Epoch 28/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2358 - binary_accuracy: 0.9272Epoch 00027: val_loss improved from 0.02168 to 0.02167, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2358 - binary_accuracy: 0.9272 - val_loss: 0.0217 - val_binary_accuracy: 0.8936
Epoch 29/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2356 - binary_accuracy: 0.9272Epoch 00028: val_loss improved from 0.02167 to 0.02165, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2356 - binary_accuracy: 0.9272 - val_loss: 0.0216 - val_binary_accuracy: 0.8936
Epoch 30/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2357 - binary_accuracy: 0.9272Epoch 00029: val_loss improved from 0.02165 to 0.02163, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2357 - binary_accuracy: 0.9272 - val_loss: 0.0216 - val_binary_accuracy: 0.8938
Epoch 31/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2356 - binary_accuracy: 0.9273Epoch 00030: val_loss improved from 0.02163 to 0.02162, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2356 - binary_accuracy: 0.9273 - val_loss: 0.0216 - val_binary_accuracy: 0.8939
Epoch 32/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2357 - binary_accuracy: 0.9272Epoch 00031: val_loss improved from 0.02162 to 0.02161, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2357 - binary_accuracy: 0.9272 - val_loss: 0.0216 - val_binary_accuracy: 0.8938
Epoch 33/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2356 - binary_accuracy: 0.9272Epoch 00032: val_loss improved from 0.02161 to 0.02157, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2356 - binary_accuracy: 0.9272 - val_loss: 0.0216 - val_binary_accuracy: 0.8937
Epoch 34/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2354 - binary_accuracy: 0.9273Epoch 00033: val_loss improved from 0.02157 to 0.02156, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 143s - loss: 0.2354 - binary_accuracy: 0.9273 - val_loss: 0.0216 - val_binary_accuracy: 0.8938
Epoch 35/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2352 - binary_accuracy: 0.9273Epoch 00034: val_loss did not improve
475000/475000 [==============================] - 143s - loss: 0.2352 - binary_accuracy: 0.9273 - val_loss: 0.0216 - val_binary_accuracy: 0.8938
Epoch 36/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2351 - binary_accuracy: 0.9273Epoch 00035: val_loss improved from 0.02156 to 0.02151, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 143s - loss: 0.2351 - binary_accuracy: 0.9273 - val_loss: 0.0215 - val_binary_accuracy: 0.8939
Epoch 37/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2352 - binary_accuracy: 0.9273Epoch 00036: val_loss improved from 0.02151 to 0.02150, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 143s - loss: 0.2352 - binary_accuracy: 0.9273 - val_loss: 0.0215 - val_binary_accuracy: 0.8939
Epoch 38/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2348 - binary_accuracy: 0.9274Epoch 00037: val_loss did not improve
475000/475000 [==============================] - 143s - loss: 0.2349 - binary_accuracy: 0.9273 - val_loss: 0.0215 - val_binary_accuracy: 0.8939
Epoch 39/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2349 - binary_accuracy: 0.9273Epoch 00038: val_loss improved from 0.02150 to 0.02147, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2349 - binary_accuracy: 0.9273 - val_loss: 0.0215 - val_binary_accuracy: 0.8940
Epoch 40/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2346 - binary_accuracy: 0.9274Epoch 00039: val_loss did not improve
475000/475000 [==============================] - 143s - loss: 0.2346 - binary_accuracy: 0.9274 - val_loss: 0.0215 - val_binary_accuracy: 0.8940
Epoch 41/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2347 - binary_accuracy: 0.9274Epoch 00040: val_loss did not improve
475000/475000 [==============================] - 143s - loss: 0.2347 - binary_accuracy: 0.9274 - val_loss: 0.0215 - val_binary_accuracy: 0.8938
Epoch 42/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2348 - binary_accuracy: 0.9274Epoch 00041: val_loss improved from 0.02147 to 0.02144, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2348 - binary_accuracy: 0.9274 - val_loss: 0.0214 - val_binary_accuracy: 0.8940
Epoch 43/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2346 - binary_accuracy: 0.9273Epoch 00042: val_loss did not improve
475000/475000 [==============================] - 143s - loss: 0.2346 - binary_accuracy: 0.9273 - val_loss: 0.0215 - val_binary_accuracy: 0.8940
Epoch 44/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2345 - binary_accuracy: 0.9274Epoch 00043: val_loss improved from 0.02144 to 0.02144, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2345 - binary_accuracy: 0.9274 - val_loss: 0.0214 - val_binary_accuracy: 0.8940
Epoch 45/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2346 - binary_accuracy: 0.9274Epoch 00044: val_loss improved from 0.02144 to 0.02140, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 143s - loss: 0.2346 - binary_accuracy: 0.9274 - val_loss: 0.0214 - val_binary_accuracy: 0.8940
Epoch 46/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2343 - binary_accuracy: 0.9274Epoch 00045: val_loss did not improve
475000/475000 [==============================] - 143s - loss: 0.2343 - binary_accuracy: 0.9274 - val_loss: 0.0214 - val_binary_accuracy: 0.8940
Epoch 47/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2343 - binary_accuracy: 0.9273Epoch 00046: val_loss did not improve
475000/475000 [==============================] - 143s - loss: 0.2344 - binary_accuracy: 0.9273 - val_loss: 0.0214 - val_binary_accuracy: 0.8940
Epoch 48/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2341 - binary_accuracy: 0.9273Epoch 00047: val_loss improved from 0.02140 to 0.02138, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2341 - binary_accuracy: 0.9273 - val_loss: 0.0214 - val_binary_accuracy: 0.8940
Epoch 49/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2342 - binary_accuracy: 0.9275Epoch 00048: val_loss did not improve
475000/475000 [==============================] - 143s - loss: 0.2342 - binary_accuracy: 0.9275 - val_loss: 0.0214 - val_binary_accuracy: 0.8940
Epoch 50/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2345 - binary_accuracy: 0.9274Epoch 00049: val_loss improved from 0.02138 to 0.02136, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2345 - binary_accuracy: 0.9274 - val_loss: 0.0214 - val_binary_accuracy: 0.8941
Epoch 51/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2342 - binary_accuracy: 0.9274Epoch 00050: val_loss improved from 0.02136 to 0.02134, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 143s - loss: 0.2342 - binary_accuracy: 0.9274 - val_loss: 0.0213 - val_binary_accuracy: 0.8941
Epoch 52/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2337 - binary_accuracy: 0.9274Epoch 00051: val_loss did not improve
475000/475000 [==============================] - 143s - loss: 0.2337 - binary_accuracy: 0.9274 - val_loss: 0.0214 - val_binary_accuracy: 0.8940
Epoch 53/100
142800/475000 [========>.....................] - ETA: 92s - loss: 0.2322 - binary_accuracy: 0.9282^Z
[1]+  Stopped                 python keras_full1.py
(python27)[minerva1993@compute-0-3 v4]$ bg %1
[1]+ python keras_full1.py &
474800/475000 [============================>.] - ETA: 0s - loss: 0.2340 - binary_accuracy: 0.9275Epoch 00052: val_loss did not improve
475000/475000 [==============================] - 156s - loss: 0.2340 - binary_accuracy: 0.9275 - val_loss: 0.0213 - val_binary_accuracy: 0.8940
Epoch 54/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2339 - binary_accuracy: 0.9274Epoch 00053: val_loss improved from 0.02134 to 0.02132, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2339 - binary_accuracy: 0.9274 - val_loss: 0.0213 - val_binary_accuracy: 0.8941
Epoch 55/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2341 - binary_accuracy: 0.9275Epoch 00054: val_loss did not improve
475000/475000 [==============================] - 143s - loss: 0.2341 - binary_accuracy: 0.9275 - val_loss: 0.0213 - val_binary_accuracy: 0.8941
Epoch 56/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2340 - binary_accuracy: 0.9274Epoch 00055: val_loss improved from 0.02132 to 0.02131, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2340 - binary_accuracy: 0.9274 - val_loss: 0.0213 - val_binary_accuracy: 0.8941
Epoch 57/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2339 - binary_accuracy: 0.9274Epoch 00056: val_loss did not improve
475000/475000 [==============================] - 144s - loss: 0.2340 - binary_accuracy: 0.9274 - val_loss: 0.0213 - val_binary_accuracy: 0.8941
Epoch 58/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2338 - binary_accuracy: 0.9273Epoch 00057: val_loss did not improve
475000/475000 [==============================] - 144s - loss: 0.2338 - binary_accuracy: 0.9273 - val_loss: 0.0213 - val_binary_accuracy: 0.8940
Epoch 59/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2337 - binary_accuracy: 0.9275Epoch 00058: val_loss did not improve
475000/475000 [==============================] - 144s - loss: 0.2337 - binary_accuracy: 0.9275 - val_loss: 0.0213 - val_binary_accuracy: 0.8942
Epoch 60/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2335 - binary_accuracy: 0.9274Epoch 00059: val_loss improved from 0.02131 to 0.02131, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 143s - loss: 0.2335 - binary_accuracy: 0.9274 - val_loss: 0.0213 - val_binary_accuracy: 0.8941
Epoch 61/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2336 - binary_accuracy: 0.9274Epoch 00060: val_loss improved from 0.02131 to 0.02130, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2336 - binary_accuracy: 0.9274 - val_loss: 0.0213 - val_binary_accuracy: 0.8941
Epoch 62/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2334 - binary_accuracy: 0.9274Epoch 00061: val_loss improved from 0.02130 to 0.02129, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2334 - binary_accuracy: 0.9274 - val_loss: 0.0213 - val_binary_accuracy: 0.8941
Epoch 63/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2338 - binary_accuracy: 0.9275Epoch 00062: val_loss improved from 0.02129 to 0.02128, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2338 - binary_accuracy: 0.9275 - val_loss: 0.0213 - val_binary_accuracy: 0.8942
Epoch 64/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2339 - binary_accuracy: 0.9275Epoch 00063: val_loss improved from 0.02128 to 0.02125, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 143s - loss: 0.2339 - binary_accuracy: 0.9275 - val_loss: 0.0213 - val_binary_accuracy: 0.8942
Epoch 65/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2335 - binary_accuracy: 0.9275Epoch 00064: val_loss improved from 0.02125 to 0.02123, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2335 - binary_accuracy: 0.9274 - val_loss: 0.0212 - val_binary_accuracy: 0.8942
Epoch 66/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2336 - binary_accuracy: 0.9274Epoch 00065: val_loss did not improve
475000/475000 [==============================] - 143s - loss: 0.2335 - binary_accuracy: 0.9274 - val_loss: 0.0213 - val_binary_accuracy: 0.8942
Epoch 67/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2333 - binary_accuracy: 0.9275Epoch 00066: val_loss did not improve
475000/475000 [==============================] - 143s - loss: 0.2333 - binary_accuracy: 0.9275 - val_loss: 0.0213 - val_binary_accuracy: 0.8942
Epoch 68/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2334 - binary_accuracy: 0.9275Epoch 00067: val_loss did not improve
475000/475000 [==============================] - 143s - loss: 0.2334 - binary_accuracy: 0.9275 - val_loss: 0.0213 - val_binary_accuracy: 0.8941
Epoch 69/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2335 - binary_accuracy: 0.9275Epoch 00068: val_loss did not improve
475000/475000 [==============================] - 143s - loss: 0.2335 - binary_accuracy: 0.9275 - val_loss: 0.0212 - val_binary_accuracy: 0.8942
Epoch 70/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2334 - binary_accuracy: 0.9274Epoch 00069: val_loss did not improve
475000/475000 [==============================] - 143s - loss: 0.2333 - binary_accuracy: 0.9274 - val_loss: 0.0212 - val_binary_accuracy: 0.8942
Epoch 71/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2332 - binary_accuracy: 0.9275Epoch 00070: val_loss did not improve
475000/475000 [==============================] - 143s - loss: 0.2332 - binary_accuracy: 0.9275 - val_loss: 0.0212 - val_binary_accuracy: 0.8942
Epoch 72/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2333 - binary_accuracy: 0.9274Epoch 00071: val_loss improved from 0.02123 to 0.02122, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 143s - loss: 0.2333 - binary_accuracy: 0.9274 - val_loss: 0.0212 - val_binary_accuracy: 0.8942
Epoch 73/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2332 - binary_accuracy: 0.9275Epoch 00072: val_loss did not improve
475000/475000 [==============================] - 143s - loss: 0.2332 - binary_accuracy: 0.9275 - val_loss: 0.0212 - val_binary_accuracy: 0.8942
Epoch 74/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2331 - binary_accuracy: 0.9275Epoch 00073: val_loss improved from 0.02122 to 0.02122, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2331 - binary_accuracy: 0.9275 - val_loss: 0.0212 - val_binary_accuracy: 0.8942
Epoch 75/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2332 - binary_accuracy: 0.9275Epoch 00074: val_loss improved from 0.02122 to 0.02121, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2332 - binary_accuracy: 0.9275 - val_loss: 0.0212 - val_binary_accuracy: 0.8942
Epoch 76/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2331 - binary_accuracy: 0.9275Epoch 00075: val_loss improved from 0.02121 to 0.02119, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2331 - binary_accuracy: 0.9275 - val_loss: 0.0212 - val_binary_accuracy: 0.8942
Epoch 77/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2333 - binary_accuracy: 0.9276Epoch 00076: val_loss did not improve
475000/475000 [==============================] - 143s - loss: 0.2333 - binary_accuracy: 0.9276 - val_loss: 0.0212 - val_binary_accuracy: 0.8942
Epoch 78/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2331 - binary_accuracy: 0.9275Epoch 00077: val_loss did not improve
475000/475000 [==============================] - 143s - loss: 0.2331 - binary_accuracy: 0.9275 - val_loss: 0.0212 - val_binary_accuracy: 0.8943
Epoch 79/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2333 - binary_accuracy: 0.9275Epoch 00078: val_loss improved from 0.02119 to 0.02119, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2333 - binary_accuracy: 0.9275 - val_loss: 0.0212 - val_binary_accuracy: 0.8943
Epoch 80/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2327 - binary_accuracy: 0.9275Epoch 00079: val_loss did not improve
475000/475000 [==============================] - 143s - loss: 0.2327 - binary_accuracy: 0.9275 - val_loss: 0.0212 - val_binary_accuracy: 0.8943
Epoch 81/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2330 - binary_accuracy: 0.9275Epoch 00080: val_loss did not improve
475000/475000 [==============================] - 143s - loss: 0.2330 - binary_accuracy: 0.9275 - val_loss: 0.0212 - val_binary_accuracy: 0.8943
Epoch 82/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2328 - binary_accuracy: 0.9276Epoch 00081: val_loss improved from 0.02119 to 0.02116, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2328 - binary_accuracy: 0.9276 - val_loss: 0.0212 - val_binary_accuracy: 0.8943
Epoch 83/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2327 - binary_accuracy: 0.9275Epoch 00082: val_loss did not improve
475000/475000 [==============================] - 144s - loss: 0.2327 - binary_accuracy: 0.9275 - val_loss: 0.0212 - val_binary_accuracy: 0.8943
Epoch 84/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2327 - binary_accuracy: 0.9275Epoch 00083: val_loss did not improve
475000/475000 [==============================] - 144s - loss: 0.2327 - binary_accuracy: 0.9275 - val_loss: 0.0212 - val_binary_accuracy: 0.8943
Epoch 85/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2328 - binary_accuracy: 0.9275Epoch 00084: val_loss did not improve
475000/475000 [==============================] - 144s - loss: 0.2328 - binary_accuracy: 0.9275 - val_loss: 0.0212 - val_binary_accuracy: 0.8942
Epoch 86/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2330 - binary_accuracy: 0.9275Epoch 00085: val_loss did not improve
475000/475000 [==============================] - 144s - loss: 0.2330 - binary_accuracy: 0.9275 - val_loss: 0.0212 - val_binary_accuracy: 0.8942
Epoch 87/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2328 - binary_accuracy: 0.9275Epoch 00086: val_loss did not improve
475000/475000 [==============================] - 145s - loss: 0.2327 - binary_accuracy: 0.9275 - val_loss: 0.0212 - val_binary_accuracy: 0.8942
Epoch 88/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2327 - binary_accuracy: 0.9275Epoch 00087: val_loss improved from 0.02116 to 0.02116, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2327 - binary_accuracy: 0.9275 - val_loss: 0.0212 - val_binary_accuracy: 0.8943
Epoch 89/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2327 - binary_accuracy: 0.9276Epoch 00088: val_loss improved from 0.02116 to 0.02114, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2327 - binary_accuracy: 0.9276 - val_loss: 0.0211 - val_binary_accuracy: 0.8943
Epoch 90/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2327 - binary_accuracy: 0.9275Epoch 00089: val_loss did not improve
475000/475000 [==============================] - 144s - loss: 0.2327 - binary_accuracy: 0.9275 - val_loss: 0.0212 - val_binary_accuracy: 0.8943
Epoch 91/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2325 - binary_accuracy: 0.9275Epoch 00090: val_loss did not improve
475000/475000 [==============================] - 144s - loss: 0.2325 - binary_accuracy: 0.9275 - val_loss: 0.0211 - val_binary_accuracy: 0.8943
Epoch 92/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2325 - binary_accuracy: 0.9276Epoch 00091: val_loss did not improve
475000/475000 [==============================] - 144s - loss: 0.2325 - binary_accuracy: 0.9275 - val_loss: 0.0212 - val_binary_accuracy: 0.8943
Epoch 93/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2326 - binary_accuracy: 0.9275Epoch 00092: val_loss did not improve
475000/475000 [==============================] - 144s - loss: 0.2326 - binary_accuracy: 0.9275 - val_loss: 0.0212 - val_binary_accuracy: 0.8943
Epoch 94/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2326 - binary_accuracy: 0.9276Epoch 00093: val_loss did not improve
475000/475000 [==============================] - 145s - loss: 0.2326 - binary_accuracy: 0.9276 - val_loss: 0.0211 - val_binary_accuracy: 0.8942
Epoch 95/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2324 - binary_accuracy: 0.9276Epoch 00094: val_loss did not improve
475000/475000 [==============================] - 144s - loss: 0.2324 - binary_accuracy: 0.9276 - val_loss: 0.0212 - val_binary_accuracy: 0.8943
Epoch 96/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2326 - binary_accuracy: 0.9276Epoch 00095: val_loss did not improve
475000/475000 [==============================] - 144s - loss: 0.2326 - binary_accuracy: 0.9276 - val_loss: 0.0211 - val_binary_accuracy: 0.8943
Epoch 97/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2325 - binary_accuracy: 0.9276Epoch 00096: val_loss improved from 0.02114 to 0.02113, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2324 - binary_accuracy: 0.9276 - val_loss: 0.0211 - val_binary_accuracy: 0.8943
Epoch 98/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2324 - binary_accuracy: 0.9276Epoch 00097: val_loss did not improve
475000/475000 [==============================] - 144s - loss: 0.2324 - binary_accuracy: 0.9276 - val_loss: 0.0211 - val_binary_accuracy: 0.8943
Epoch 99/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2325 - binary_accuracy: 0.9275Epoch 00098: val_loss improved from 0.02113 to 0.02113, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 145s - loss: 0.2325 - binary_accuracy: 0.9275 - val_loss: 0.0211 - val_binary_accuracy: 0.8943
Epoch 100/100
474800/475000 [============================>.] - ETA: 0s - loss: 0.2326 - binary_accuracy: 0.9276Epoch 00099: val_loss improved from 0.02113 to 0.02112, saving model to keras4_Hct2/weights/TrainedModel_PyKeras.h5
475000/475000 [==============================] - 144s - loss: 0.2326 - binary_accuracy: 0.9276 - val_loss: 0.0211 - val_binary_accuracy: 0.8943
                         : Elapsed time for training with 475000 events: 1.45e+04 sec         
                         : Creating xml weight file: keras4_Hct2/weights/TMVAClassification_PyKeras.weights.xml
                         : Creating standalone class: keras4_Hct2/weights/TMVAClassification_PyKeras.class.C
Factory                  : Training finished
                         : 
                         : Ranking input variables (method specific)...
BDT                      : Ranking result (top variable is best ranked)
                         : ----------------------------------------------
                         : Rank : Variable     : Variable Importance
                         : ----------------------------------------------
                         :    1 : DRjet2cvsb   : 2.311e-02
                         :    2 : DRjet12m     : 2.274e-02
                         :    3 : DRjet3cvsl   : 2.159e-02
                         :    4 : DRjet3csv    : 1.942e-02
                         :    5 : DRjet2csv    : 1.917e-02
                         :    6 : DRjet0csv    : 1.896e-02
                         :    7 : DRjet0cvsb   : 1.821e-02
                         :    8 : DRjet23m     : 1.809e-02
                         :    9 : DRjet12DR    : 1.745e-02
                         :   10 : DRjet31m     : 1.713e-02
                         :   11 : DRjet2cvsl   : 1.667e-02
                         :   12 : DRjet3m      : 1.640e-02
                         :   13 : DRjet1cvsb   : 1.611e-02
                         :   14 : DRjet0cvsl   : 1.565e-02
                         :   15 : jet1cvsb     : 1.490e-02
                         :   16 : DRjet1csv    : 1.474e-02
                         :   17 : DRjet0eta    : 1.473e-02
                         :   18 : DRjet3cvsb   : 1.467e-02
                         :   19 : jet4cvsl     : 1.462e-02
                         :   20 : jet4cvsb     : 1.427e-02
                         :   21 : jet2cvsb     : 1.414e-02
                         :   22 : jet3cvsb     : 1.360e-02
                         :   23 : jet3csv      : 1.339e-02
                         :   24 : jet3cvsl     : 1.283e-02
                         :   25 : nbjets_m     : 1.280e-02
                         :   26 : DRjet2eta    : 1.273e-02
                         :   27 : jet3eta      : 1.262e-02
                         :   28 : DRlepTdphi   : 1.259e-02
                         :   29 : DRhadTm      : 1.253e-02
                         :   30 : DRlepTm      : 1.252e-02
                         :   31 : DRjet12dphi  : 1.229e-02
                         :   32 : DRjet1m      : 1.221e-02
                         :   33 : DRlepWdphi   : 1.209e-02
                         :   34 : DRlepTpt     : 1.202e-02
                         :   35 : DRhadTHbdphi : 1.202e-02
                         :   36 : jet2eta      : 1.201e-02
                         :   37 : jet4csv      : 1.198e-02
                         :   38 : jet2cvsl     : 1.195e-02
                         :   39 : jet1pt       : 1.186e-02
                         :   40 : njets        : 1.164e-02
                         :   41 : jet1cvsl     : 1.151e-02
                         :   42 : jet4eta      : 1.146e-02
                         :   43 : jet1eta      : 1.131e-02
                         :   44 : DRjet31dphi  : 1.129e-02
                         :   45 : DRjet2m      : 1.126e-02
                         :   46 : DRjet0m      : 1.108e-02
                         :   47 : DRjet23dphi  : 1.105e-02
                         :   48 : DRjet3eta    : 1.099e-02
                         :   49 : jet2csv      : 1.095e-02
                         :   50 : DRlepWm      : 1.089e-02
                         :   51 : DRhadTWbdphi : 1.056e-02
                         :   52 : jet1csv      : 1.054e-02
                         :   53 : DRjet23deta  : 1.048e-02
                         :   54 : DRjet1cvsl   : 1.044e-02
                         :   55 : DRjet3pt     : 1.039e-02
                         :   56 : DRjet1eta    : 1.012e-02
                         :   57 : DRjet31deta  : 9.948e-03
                         :   58 : DRjet23pt    : 9.916e-03
                         :   59 : DRlepWeta    : 9.645e-03
                         :   60 : DRlepWdeta   : 9.561e-03
                         :   61 : DRjet12deta  : 9.068e-03
                         :   62 : DRhadTpt     : 8.872e-03
                         :   63 : DRjet0pt     : 8.754e-03
                         :   64 : DRlepTeta    : 8.615e-03
                         :   65 : jet1m        : 8.611e-03
                         :   66 : DRjet12eta   : 8.591e-03
                         :   67 : DRlepWpt     : 8.394e-03
                         :   68 : DRlepTdeta   : 8.210e-03
                         :   69 : DRjet12pt    : 7.865e-03
                         :   70 : jet2m        : 7.809e-03
                         :   71 : DRhadTHbdeta : 7.725e-03
                         :   72 : DRjet23eta   : 7.684e-03
                         :   73 : jet3pt       : 7.683e-03
                         :   74 : jet4pt       : 7.513e-03
                         :   75 : DRhadTeta    : 7.432e-03
                         :   76 : DRhadTWbdeta : 7.391e-03
                         :   77 : jet3m        : 7.389e-03
                         :   78 : jet4m        : 7.272e-03
                         :   79 : jet2pt       : 7.200e-03
                         :   80 : DRjet31eta   : 6.770e-03
                         :   81 : ncjets_m     : 6.556e-03
                         :   82 : DRjet31pt    : 6.116e-03
                         :   83 : DRjet2pt     : 6.103e-03
                         :   84 : missingET    : 5.739e-03
                         :   85 : DRjet1pt     : 4.803e-03
                         : ----------------------------------------------
                         : No variable ranking supplied by classifier: PyKeras
Factory                  : === Destroy and recreate all methods via weight files for testing ===
                         : 
Factory                  : Test all methods
Factory                  : Test method: BDT for Classification performance
                         : 
BDT                      : [keras4_Hct2] : Evaluation of BDT on testing sample (187095 events)
                         : Elapsed time for evaluation of 187095 events: 9.9 sec       
Factory                  : Test method: PyKeras for Classification performance
                         : 
                         : Load model from file: keras4_Hct2/weights/TrainedModel_PyKeras.h5
Factory                  : Evaluate all methods
Factory                  : Evaluate classifier: BDT
                         : 
BDT                      : [keras4_Hct2] : Loop over test events and fill histograms with classifier response...
                         : 
TFHandler_BDT            :     Variable            Mean            RMS    [        Min            Max ]
                         : -------------------------------------------------------------------------------
                         :        njets:        5.0041        1.1425   [        4.0000        16.000 ]
                         :     nbjets_m:        3.0527       0.23450   [        3.0000        7.0000 ]
                         :     ncjets_m:       0.73865       0.81211   [        0.0000        7.0000 ]
                         :    missingET:        69.377        50.616   [       0.16872        809.45 ]
                         :       jet1pt:        148.92        92.238   [        34.631        1486.1 ]
                         :      jet1eta:      0.010767        1.0744   [       -2.3999        2.3996 ]
                         :        jet1m:        258.34        225.88   [        37.981        3764.0 ]
                         :      jet1csv:       0.74139       0.33342   [      0.027290       0.99963 ]
                         :     jet1cvsl:       0.52564       0.49427   [      -0.74829       0.99745 ]
                         :     jet1cvsb:      -0.23534       0.52326   [      -0.98582       0.85683 ]
                         :       jet2pt:        100.48        54.914   [        30.690        1405.2 ]
                         :      jet2eta:    -0.0072095        1.0793   [       -2.4000        2.4000 ]
                         :        jet2m:        173.00        136.97   [        33.022        3038.1 ]
                         :      jet2csv:       0.74106       0.33539   [      0.034453       0.99962 ]
                         :     jet2cvsl:       0.54599       0.49531   [      -0.75300       0.99742 ]
                         :     jet2cvsb:      -0.24639       0.52851   [      -0.98564       0.84497 ]
                         :       jet3pt:        71.578        33.627   [        30.118        1011.1 ]
                         :      jet3eta:    -0.0019896        1.1087   [       -2.4000        2.3999 ]
                         :        jet3m:        126.46        93.405   [        30.613        2008.2 ]
                         :      jet3csv:       0.70362       0.34829   [      0.039417        1.0000 ]
                         :     jet3cvsl:       0.49810       0.51627   [      -0.82375       0.99753 ]
                         :     jet3cvsb:      -0.18990       0.53446   [      -0.98892       0.85355 ]
                         :       jet4pt:        52.881        22.879   [        30.000        664.07 ]
                         :      jet4eta:    0.00084081        1.1462   [       -2.3999        2.4000 ]
                         :        jet4m:        96.743        69.077   [        30.293        1315.0 ]
                         :      jet4csv:       0.66817       0.35182   [      0.033368       0.99963 ]
                         :     jet4cvsl:       0.43587       0.52902   [      -0.81260       0.99722 ]
                         :     jet4cvsb:      -0.13126       0.52818   [      -0.98686       0.84968 ]
                         :     DRlepWpt:        116.73        77.272   [      0.036197        1061.2 ]
                         :    DRlepWeta:     0.0075814       0.87133   [       -8.3304        6.1079 ]
                         :   DRlepWdeta:     0.0084492       0.97564   [       -6.0616        6.0283 ]
                         :   DRlepWdphi:    -0.0063482        1.5989   [       -3.1416        3.1415 ]
                         :      DRlepWm:        94.396        48.824   [       0.36002        703.57 ]
                         :     DRjet0pt:        117.26        90.950   [        30.000        1421.6 ]
                         :    DRjet0eta:     0.0027506        1.1775   [       -2.3999        2.4000 ]
                         :      DRjet0m:        15.550        10.632   [        1.5900        273.92 ]
                         :    DRjet0csv:       0.66428       0.35960   [      0.027290       0.99962 ]
                         :   DRjet0cvsl:       0.42904       0.52674   [      -0.74829       0.99712 ]
                         :   DRjet0cvsb:      -0.14627       0.54510   [      -0.98655       0.84300 ]
                         :     DRjet1pt:        83.010        53.788   [        30.000        1055.3 ]
                         :    DRjet1eta:     0.0048816        1.0088   [       -2.3999        2.4000 ]
                         :      DRjet1m:        12.058        6.9273   [        1.6639        222.72 ]
                         :    DRjet1csv:       0.97480      0.030573   [       0.84890        1.0000 ]
                         :   DRjet1cvsl:       0.86833       0.17828   [      -0.61461       0.99761 ]
                         :   DRjet1cvsb:      -0.68784       0.27375   [      -0.98892       0.77252 ]
                         :     DRjet2pt:        81.450        58.111   [        30.000        1343.1 ]
                         :    DRjet2eta:    -0.0031566        1.0697   [       -2.4000        2.4000 ]
                         :      DRjet2m:        11.933        7.8106   [       0.72645        196.18 ]
                         :    DRjet2csv:       0.92207      0.044924   [       0.84840       0.99943 ]
                         :   DRjet2cvsl:       0.78815       0.24529   [      -0.69801       0.99753 ]
                         :   DRjet2cvsb:      -0.35723       0.34765   [      -0.98621       0.81418 ]
                         :     DRjet3pt:        73.525        52.322   [        30.000        1486.1 ]
                         :    DRjet3eta:   -0.00029648        1.0816   [       -2.4000        2.3998 ]
                         :      DRjet3m:        10.990        6.8610   [       0.42219        177.84 ]
                         :    DRjet3csv:       0.39904       0.30961   [      0.033368       0.99868 ]
                         :   DRjet3cvsl:      0.057206       0.45829   [      -0.82375       0.99704 ]
                         :   DRjet3cvsb:       0.25653       0.35096   [      -0.98164       0.86836 ]
                         :    DRjet12pt:        138.01        77.923   [       0.69027        1541.4 ]
                         :   DRjet12eta:     0.0021573        1.1533   [       -6.2902        6.1921 ]
                         :  DRjet12deta:     0.0072976        1.2066   [       -7.5324        6.5640 ]
                         :  DRjet12dphi:    0.00014111        1.2929   [       -3.1414        3.1411 ]
                         :     DRjet12m:        112.23        64.498   [        15.694        1311.4 ]
                         :    DRjet12DR:        1.5158       0.63021   [       0.38371        4.8788 ]
                         :    DRjet23pt:        124.22        79.472   [       0.44326        1513.4 ]
                         :   DRjet23eta:    -0.0013729        1.2799   [       -6.1839        6.6104 ]
                         :  DRjet23deta:   -0.00058632        1.1796   [       -6.4313        6.6578 ]
                         :  DRjet23dphi:     0.0026183        1.4941   [       -3.1414        3.1416 ]
                         :     DRjet23m:        110.32        71.680   [        15.268        2128.1 ]
                         :    DRjet31pt:        123.79        76.934   [       0.29825        1523.5 ]
                         :   DRjet31eta:     0.0044624        1.2537   [       -6.6529        6.6030 ]
                         :  DRjet31deta:     0.0041109        1.1603   [       -6.9280        5.8994 ]
                         :  DRjet31dphi:    -0.0062853        1.5254   [       -3.1416        3.1414 ]
                         :     DRjet31m:        113.59        67.764   [        16.360        1838.6 ]
                         :     DRlepTpt:        163.27        98.876   [       0.31991        1328.8 ]
                         :    DRlepTeta:     0.0078198        1.2330   [       -6.7489        7.0918 ]
                         :   DRlepTdeta:     0.0064629        1.1245   [       -7.3325        6.6299 ]
                         :   DRlepTdphi:     0.0092824        1.7647   [       -3.1415        3.1416 ]
                         :      DRlepTm:        261.36        163.17   [        27.131        2772.9 ]
                         :     DRhadTpt:        175.76        96.335   [        1.1789        1606.8 ]
                         :    DRhadTeta:     0.0019500        1.2601   [       -7.2177        5.9549 ]
                         : DRhadTHbdeta:     0.0031095        1.1487   [       -7.3813        6.3473 ]
                         : DRhadTWbdeta:    -0.0027509        1.2476   [       -6.7525        6.7300 ]
                         : DRhadTHbdphi:    -0.0021514        1.4216   [       -3.1413        3.1415 ]
                         : DRhadTWbdphi:   -0.00011182        1.3128   [       -3.1414        3.1415 ]
                         :      DRhadTm:        203.12        98.653   [        32.554        2209.8 ]
                         : -------------------------------------------------------------------------------
                         : 
                         : <PlotVariables> Will not produce scatter plots ==> 
                         : |  The number of 85 input variables and 0 target values would require 3570 two-dimensional
                         : |  histograms, which would occupy the computer's memory. Note that this
                         : |  suppression does not have any consequences for your analysis, other
                         : |  than not disposing of these scatter plots. You can modify the maximum
                         : |  number of input variables allowed to generate scatter plots in your
                         : |  script via the command line:
                         : |  "(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;"
                         : 
                         : Some more output
Factory                  : Evaluate classifier: PyKeras
                         : 
TFHandler_PyKeras        :     Variable            Mean            RMS    [        Min            Max ]
                         : -------------------------------------------------------------------------------
                         :        njets:     0.0019986       0.99472   [       -3.6027        5.7307 ]
                         :     nbjets_m:   -0.00029524       0.99453   [       -3.2251        3.9017 ]
                         :     ncjets_m:     0.0012626       0.99851   [       -3.4234        5.7307 ]
                         :    missingET:    -0.0019917       0.99672   [       -3.0330        4.0123 ]
                         :       jet1pt:     0.0035024       0.99824   [       -3.0661        5.7307 ]
                         :      jet1eta:      0.011206       0.99998   [       -3.2810        5.7307 ]
                         :        jet1m:    0.00077789       0.99777   [       -4.0281        4.0033 ]
                         :      jet1csv:     0.0045662       0.99523   [       -3.3427        5.7307 ]
                         :     jet1cvsl:     0.0019753       0.99316   [       -3.1204        5.7307 ]
                         :     jet1cvsb:     0.0016365       0.99012   [       -3.3532        5.7307 ]
                         :       jet2pt:     0.0064046       0.99732   [       -3.5441        5.7307 ]
                         :      jet2eta:    -0.0045894        1.0002   [       -3.1301        5.7307 ]
                         :        jet2m:     0.0064972       0.99535   [       -3.2827        5.7307 ]
                         :      jet2csv:     0.0014740       0.99808   [       -3.1314        5.7307 ]
                         :     jet2cvsl:     0.0033153       0.99921   [       -3.5937        5.7307 ]
                         :     jet2cvsb:     0.0055488       0.99524   [       -3.3129        5.7307 ]
                         :       jet3pt:     0.0023339       0.99391   [       -3.1919        5.7307 ]
                         :      jet3eta:    -0.0039564       0.99575   [       -3.5641        5.7307 ]
                         :        jet3m:     0.0022852       0.99766   [       -3.1998        5.7307 ]
                         :      jet3csv:     0.0062845       0.99228   [       -3.3586        3.8388 ]
                         :     jet3cvsl:     0.0082144       0.99656   [       -3.1228        5.7307 ]
                         :     jet3cvsb:     0.0016925       0.99228   [       -3.3360        5.7307 ]
                         :       jet4pt:    -0.0050693       0.99821   [       -5.7307        5.7307 ]
                         :      jet4eta:     0.0059067       0.99783   [       -3.2008        3.8821 ]
                         :        jet4m:     0.0014183       0.99628   [       -3.7040        5.7307 ]
                         :      jet4csv:     0.0039784       0.99497   [       -3.4373        5.7307 ]
                         :     jet4cvsl:    -0.0039179       0.99445   [       -3.3617        5.7307 ]
                         :     jet4cvsb:     0.0051616       0.99651   [       -3.1624        5.7307 ]
                         :     DRlepWpt:     0.0026664       0.99837   [       -3.1454        3.5356 ]
                         :    DRlepWeta:   -0.00018708       0.99192   [       -5.7307        3.7832 ]
                         :   DRlepWdeta:     0.0029708       0.98940   [       -3.2069        3.4851 ]
                         :   DRlepWdphi:    -0.0031439        1.0011   [       -5.7307        5.7307 ]
                         :      DRlepWm:     0.0022280       0.99346   [       -3.1119        3.8496 ]
                         :     DRjet0pt:    -0.0010460       0.99595   [       -3.3729        5.7307 ]
                         :    DRjet0eta:     0.0070290       0.99638   [       -3.2087        5.7307 ]
                         :      DRjet0m:    -0.0055180        1.0003   [       -3.1355        5.7307 ]
                         :    DRjet0csv:    0.00053530       0.99381   [       -3.2140        3.7977 ]
                         :   DRjet0cvsl:     0.0071489       0.99860   [       -3.1153        5.7307 ]
                         :   DRjet0cvsb:     0.0037167       0.99473   [       -3.2094        5.7307 ]
                         :     DRjet1pt:     0.0052275       0.99922   [       -3.3623        5.7307 ]
                         :    DRjet1eta:     0.0064614       0.99996   [       -3.1895        5.7307 ]
                         :      DRjet1m:     0.0059037       0.99941   [       -3.4004        5.7307 ]
                         :    DRjet1csv:     0.0039513       0.99954   [       -3.2462        5.7307 ]
                         :   DRjet1cvsl:     0.0091335        1.0046   [       -3.1287        5.7307 ]
                         :   DRjet1cvsb:     0.0070070       0.99718   [       -5.7307        5.7307 ]
                         :     DRjet2pt:    -0.0022117       0.99680   [       -3.0802        5.7307 ]
                         :    DRjet2eta:    0.00037470       0.99875   [       -3.3921        5.7307 ]
                         :      DRjet2m:     0.0036650       0.99594   [       -3.0328        5.7307 ]
                         :    DRjet2csv:   -0.00026833       0.99657   [       -3.3624        5.7307 ]
                         :   DRjet2cvsl:     0.0058028       0.99700   [       -3.3124        5.7307 ]
                         :   DRjet2cvsb:    -0.0015807       0.99621   [       -3.5623        5.7307 ]
                         :     DRjet3pt:     0.0023787       0.99416   [       -3.0651        5.7307 ]
                         :    DRjet3eta:    0.00078630       0.99399   [       -3.1068        5.7307 ]
                         :      DRjet3m:     0.0068494       0.99648   [       -5.7307        5.7307 ]
                         :    DRjet3csv:     0.0071061       0.99390   [       -3.3546        5.7307 ]
                         :   DRjet3cvsl:   -0.00056556       0.99227   [       -3.2033        5.7307 ]
                         :   DRjet3cvsb:    0.00045454       0.99246   [       -3.5204        5.7307 ]
                         :    DRjet12pt:     0.0077128       0.99407   [       -3.0323        5.7307 ]
                         :   DRjet12eta:     0.0045025       0.99374   [       -3.3051        5.7307 ]
                         :  DRjet12deta:     0.0041950       0.99828   [       -3.2412        5.7307 ]
                         :  DRjet12dphi:     0.0036503       0.99292   [       -3.1980        5.7307 ]
                         :     DRjet12m:    -0.0019614       0.99754   [       -3.1802        5.7307 ]
                         :    DRjet12DR:    -0.0017183       0.99530   [       -3.0930        5.7307 ]
                         :    DRjet23pt:    -0.0036779       0.99474   [       -3.0987        3.4652 ]
                         :   DRjet23eta:     0.0043520       0.99746   [       -3.1629        5.7307 ]
                         :  DRjet23deta:     0.0080202       0.99644   [       -3.3545        5.7307 ]
                         :  DRjet23dphi:     0.0046252       0.99691   [       -3.1382        5.7307 ]
                         :     DRjet23m:     0.0025010       0.99091   [       -3.4475        5.7307 ]
                         :    DRjet31pt:     0.0020237       0.99924   [       -3.1166        5.7307 ]
                         :   DRjet31eta:     0.0027210       0.99680   [       -3.3702        5.7307 ]
                         :  DRjet31deta:     0.0015071       0.99581   [       -3.1488        5.7307 ]
                         :  DRjet31dphi:    -0.0016515       0.99423   [       -3.2038        5.7307 ]
                         :     DRjet31m:     0.0012068       0.99372   [       -3.3636        5.7307 ]
                         :     DRlepTpt:     0.0074007       0.99500   [       -5.7307        5.7307 ]
                         :    DRlepTeta:     0.0063629       0.99191   [       -3.3439        5.7307 ]
                         :   DRlepTdeta:     0.0049583        1.0020   [       -3.2901        3.6374 ]
                         :   DRlepTdphi:     0.0064048       0.99076   [       -3.2077        5.7307 ]
                         :      DRlepTm:     0.0038251       0.99506   [       -3.1283        3.7310 ]
                         :     DRhadTpt:     0.0057925       0.99500   [       -3.1965        5.7307 ]
                         :    DRhadTeta:     0.0046095       0.99443   [       -3.1729        5.7307 ]
                         : DRhadTHbdeta:     0.0051458       0.99752   [       -3.4283        5.7307 ]
                         : DRhadTWbdeta:     0.0072668       0.99830   [       -3.1551        5.7307 ]
                         : DRhadTHbdphi:     0.0032155       0.99440   [       -3.3758        5.7307 ]
                         : DRhadTWbdphi:    0.00014019       0.99272   [       -3.2335        4.0858 ]
                         :      DRhadTm:    -0.0049947        1.0034   [       -3.1463        5.7307 ]
                         : -------------------------------------------------------------------------------
PyKeras                  : [keras4_Hct2] : Loop over test events and fill histograms with classifier response...
                         : 
TFHandler_PyKeras        :     Variable            Mean            RMS    [        Min            Max ]
                         : -------------------------------------------------------------------------------
                         :        njets:     0.0019986       0.99472   [       -3.6027        5.7307 ]
                         :     nbjets_m:   -0.00029524       0.99453   [       -3.2251        3.9017 ]
                         :     ncjets_m:     0.0012626       0.99851   [       -3.4234        5.7307 ]
                         :    missingET:    -0.0019917       0.99672   [       -3.0330        4.0123 ]
                         :       jet1pt:     0.0035024       0.99824   [       -3.0661        5.7307 ]
                         :      jet1eta:      0.011206       0.99998   [       -3.2810        5.7307 ]
                         :        jet1m:    0.00077789       0.99777   [       -4.0281        4.0033 ]
                         :      jet1csv:     0.0045662       0.99523   [       -3.3427        5.7307 ]
                         :     jet1cvsl:     0.0019753       0.99316   [       -3.1204        5.7307 ]
                         :     jet1cvsb:     0.0016365       0.99012   [       -3.3532        5.7307 ]
                         :       jet2pt:     0.0064046       0.99732   [       -3.5441        5.7307 ]
                         :      jet2eta:    -0.0045894        1.0002   [       -3.1301        5.7307 ]
                         :        jet2m:     0.0064972       0.99535   [       -3.2827        5.7307 ]
                         :      jet2csv:     0.0014740       0.99808   [       -3.1314        5.7307 ]
                         :     jet2cvsl:     0.0033153       0.99921   [       -3.5937        5.7307 ]
                         :     jet2cvsb:     0.0055488       0.99524   [       -3.3129        5.7307 ]
                         :       jet3pt:     0.0023339       0.99391   [       -3.1919        5.7307 ]
                         :      jet3eta:    -0.0039564       0.99575   [       -3.5641        5.7307 ]
                         :        jet3m:     0.0022852       0.99766   [       -3.1998        5.7307 ]
                         :      jet3csv:     0.0062845       0.99228   [       -3.3586        3.8388 ]
                         :     jet3cvsl:     0.0082144       0.99656   [       -3.1228        5.7307 ]
                         :     jet3cvsb:     0.0016925       0.99228   [       -3.3360        5.7307 ]
                         :       jet4pt:    -0.0050693       0.99821   [       -5.7307        5.7307 ]
                         :      jet4eta:     0.0059067       0.99783   [       -3.2008        3.8821 ]
                         :        jet4m:     0.0014183       0.99628   [       -3.7040        5.7307 ]
                         :      jet4csv:     0.0039784       0.99497   [       -3.4373        5.7307 ]
                         :     jet4cvsl:    -0.0039179       0.99445   [       -3.3617        5.7307 ]
                         :     jet4cvsb:     0.0051616       0.99651   [       -3.1624        5.7307 ]
                         :     DRlepWpt:     0.0026664       0.99837   [       -3.1454        3.5356 ]
                         :    DRlepWeta:   -0.00018708       0.99192   [       -5.7307        3.7832 ]
                         :   DRlepWdeta:     0.0029708       0.98940   [       -3.2069        3.4851 ]
                         :   DRlepWdphi:    -0.0031439        1.0011   [       -5.7307        5.7307 ]
                         :      DRlepWm:     0.0022280       0.99346   [       -3.1119        3.8496 ]
                         :     DRjet0pt:    -0.0010460       0.99595   [       -3.3729        5.7307 ]
                         :    DRjet0eta:     0.0070290       0.99638   [       -3.2087        5.7307 ]
                         :      DRjet0m:    -0.0055180        1.0003   [       -3.1355        5.7307 ]
                         :    DRjet0csv:    0.00053530       0.99381   [       -3.2140        3.7977 ]
                         :   DRjet0cvsl:     0.0071489       0.99860   [       -3.1153        5.7307 ]
                         :   DRjet0cvsb:     0.0037167       0.99473   [       -3.2094        5.7307 ]
                         :     DRjet1pt:     0.0052275       0.99922   [       -3.3623        5.7307 ]
                         :    DRjet1eta:     0.0064614       0.99996   [       -3.1895        5.7307 ]
                         :      DRjet1m:     0.0059037       0.99941   [       -3.4004        5.7307 ]
                         :    DRjet1csv:     0.0039513       0.99954   [       -3.2462        5.7307 ]
                         :   DRjet1cvsl:     0.0091335        1.0046   [       -3.1287        5.7307 ]
                         :   DRjet1cvsb:     0.0070070       0.99718   [       -5.7307        5.7307 ]
                         :     DRjet2pt:    -0.0022117       0.99680   [       -3.0802        5.7307 ]
                         :    DRjet2eta:    0.00037470       0.99875   [       -3.3921        5.7307 ]
                         :      DRjet2m:     0.0036650       0.99594   [       -3.0328        5.7307 ]
                         :    DRjet2csv:   -0.00026833       0.99657   [       -3.3624        5.7307 ]
                         :   DRjet2cvsl:     0.0058028       0.99700   [       -3.3124        5.7307 ]
                         :   DRjet2cvsb:    -0.0015807       0.99621   [       -3.5623        5.7307 ]
                         :     DRjet3pt:     0.0023787       0.99416   [       -3.0651        5.7307 ]
                         :    DRjet3eta:    0.00078630       0.99399   [       -3.1068        5.7307 ]
                         :      DRjet3m:     0.0068494       0.99648   [       -5.7307        5.7307 ]
                         :    DRjet3csv:     0.0071061       0.99390   [       -3.3546        5.7307 ]
                         :   DRjet3cvsl:   -0.00056556       0.99227   [       -3.2033        5.7307 ]
                         :   DRjet3cvsb:    0.00045454       0.99246   [       -3.5204        5.7307 ]
                         :    DRjet12pt:     0.0077128       0.99407   [       -3.0323        5.7307 ]
                         :   DRjet12eta:     0.0045025       0.99374   [       -3.3051        5.7307 ]
                         :  DRjet12deta:     0.0041950       0.99828   [       -3.2412        5.7307 ]
                         :  DRjet12dphi:     0.0036503       0.99292   [       -3.1980        5.7307 ]
                         :     DRjet12m:    -0.0019614       0.99754   [       -3.1802        5.7307 ]
                         :    DRjet12DR:    -0.0017183       0.99530   [       -3.0930        5.7307 ]
                         :    DRjet23pt:    -0.0036779       0.99474   [       -3.0987        3.4652 ]
                         :   DRjet23eta:     0.0043520       0.99746   [       -3.1629        5.7307 ]
                         :  DRjet23deta:     0.0080202       0.99644   [       -3.3545        5.7307 ]
                         :  DRjet23dphi:     0.0046252       0.99691   [       -3.1382        5.7307 ]
                         :     DRjet23m:     0.0025010       0.99091   [       -3.4475        5.7307 ]
                         :    DRjet31pt:     0.0020237       0.99924   [       -3.1166        5.7307 ]
                         :   DRjet31eta:     0.0027210       0.99680   [       -3.3702        5.7307 ]
                         :  DRjet31deta:     0.0015071       0.99581   [       -3.1488        5.7307 ]
                         :  DRjet31dphi:    -0.0016515       0.99423   [       -3.2038        5.7307 ]
                         :     DRjet31m:     0.0012068       0.99372   [       -3.3636        5.7307 ]
                         :     DRlepTpt:     0.0074007       0.99500   [       -5.7307        5.7307 ]
                         :    DRlepTeta:     0.0063629       0.99191   [       -3.3439        5.7307 ]
                         :   DRlepTdeta:     0.0049583        1.0020   [       -3.2901        3.6374 ]
                         :   DRlepTdphi:     0.0064048       0.99076   [       -3.2077        5.7307 ]
                         :      DRlepTm:     0.0038251       0.99506   [       -3.1283        3.7310 ]
                         :     DRhadTpt:     0.0057925       0.99500   [       -3.1965        5.7307 ]
                         :    DRhadTeta:     0.0046095       0.99443   [       -3.1729        5.7307 ]
                         : DRhadTHbdeta:     0.0051458       0.99752   [       -3.4283        5.7307 ]
                         : DRhadTWbdeta:     0.0072668       0.99830   [       -3.1551        5.7307 ]
                         : DRhadTHbdphi:     0.0032155       0.99440   [       -3.3758        5.7307 ]
                         : DRhadTWbdphi:    0.00014019       0.99272   [       -3.2335        4.0858 ]
                         :      DRhadTm:    -0.0049947        1.0034   [       -3.1463        5.7307 ]
                         : -------------------------------------------------------------------------------
                         : 
                         : <PlotVariables> Will not produce scatter plots ==> 
                         : |  The number of 85 input variables and 0 target values would require 3570 two-dimensional
                         : |  histograms, which would occupy the computer's memory. Note that this
                         : |  suppression does not have any consequences for your analysis, other
                         : |  than not disposing of these scatter plots. You can modify the maximum
                         : |  number of input variables allowed to generate scatter plots in your
                         : |  script via the command line:
                         : |  "(TMVA::gConfig().GetVariablePlotting()).fMaxNumOfAllowedVariablesForScatterPlots = <some int>;"
                         : 
                         : Some more output
                         : 
                         : Evaluation results ranked by best signal efficiency and purity (area)
                         : -------------------------------------------------------------------------------------------------------------------
                         : DataSet       MVA                       
                         : Name:         Method:          ROC-integ
                         : keras4_Hct2   BDT            : 0.745
                         : keras4_Hct2   PyKeras        : 0.740
                         : -------------------------------------------------------------------------------------------------------------------
                         : 
                         : Testing efficiency compared to training efficiency (overtraining check)
                         : -------------------------------------------------------------------------------------------------------------------
                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) 
                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   
                         : -------------------------------------------------------------------------------------------------------------------
                         : keras4_Hct2          BDT            : 0.098 (0.108)       0.394 (0.394)      0.659 (0.661)
                         : keras4_Hct2          PyKeras        : 0.121 (0.122)       0.387 (0.393)      0.651 (0.654)
                         : -------------------------------------------------------------------------------------------------------------------
                         : 
Dataset:keras4_Hct2      : Created tree 'TestTree' with 187095 events
                         : 
Dataset:keras4_Hct2      : Created tree 'TrainTree' with 475000 events
